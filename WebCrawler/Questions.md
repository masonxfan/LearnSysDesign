先看spider的目的都有啥
有搜索
图书馆
数据分析
检测
其实在crawler这部分没啥不一样
我觉得更有效的问题是：
spider都需要什么功能：
手动trigger，还是cronjob形式？
spider拿到的数据是否需要存储？
存储的话：生数据？格式化的数据？
拿数据时候，是否需要filter，还是随便拿？
数据是否需要去重
拿的数据是什么格式的，网页？关键词？图片？视频？

万年好问题：数据量

数据源改变了，是不是需要更新？
这个是个好问题呐