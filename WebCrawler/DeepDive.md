DFS 还是BFS？
还是BFS，然后设置深度limit
dfs的话，可能会挖得非常深，但只有一个网页，就尴尬了

爬虫要讲礼貌，不然会被识别成dos

url selector，可以识别优先级高的url

爬虫还要反复爬，这块也可以要设计多复杂就能多复杂
比如怎么设置选择爬从前的网页，加不加queue啥的

Robots.txt Robots Exclusion Protocol 是一个html爬虫协议，保证自己爬的东西能下载下来

还可以分布式爬
从queue里拉url出来去爬

防止掉进洞里

filter呀，正则呀，白名单，花样很多：降噪

存储数据量特别大，pb级别的存储应该用什么？
看了一下，s3，每pb要2-3万吧
减少数据量还是更靠谱一些，其他的操作都贵得很